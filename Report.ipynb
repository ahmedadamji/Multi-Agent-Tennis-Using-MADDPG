{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the learning algorithm and implementation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "The Tennis environment is a multi-agent environment where two agents control rackets to bounce a ball over a net. The objective of each agent is to keep the ball in play for as long as possible. The agents receive a reward of +0.1 for hitting the ball over the net and a reward of -0.01 if the ball hits the ground or goes out of bounds. The task is considered solved when the agents get an average score of +0.5 over 100 consecutive episodes, with the score being the maximum of the agents' scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "To solve the Tennis environment, the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm has been implemented, which extends the Deep Deterministic Policy Gradient (DDPG) algorithm to the multi-agent setting. In MADDPG, each agent maintains its own actor and critic networks, which are trained using a centralized training scheme. Specifically, the critic network takes as input the states and actions of all agents, while each actor network takes as input the state of its respective agent only since an agent can only directly observe its own state. This is because each agent's actions affect the environment, which in turn affects the other agents. By including the actions of all agents in the critic's input, the agent can learn to optimize its own actions in the context of the other agents' actions.\n",
    "\n",
    "For each timestep in the episode, each agent selects an action using its actor network and adds some noise to the action for exploration using the Ornstein-Uhlenbeck process. The critic networks are trained to minimize the mean squared error between the predicted Q-value and the target Q-value, which is computed using the Bellman equation. Meanwhile, the actor networks are trained to maximize the Q-value predicted by the critic network for each action taken by the respective agent.\n",
    "\n",
    "To update the target networks, a soft update scheme is used, where the parameters of the target networks are slowly adjusted towards the parameters of the online networks.\n",
    "\n",
    "Once the training is complete, the trained Actor network can then be used to deterministically predict the action for each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome\n",
    "the MADDPG algorithm was able to solve the Tennis environment within 1968 episodes, achieving an average score of +0.5 over 100 consecutive episodes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture of the implemented neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both agents were trained on the same network class, as two separate objects.\n",
    "The implemented neural network contains two classes, Actor and Critic.\n",
    "\n",
    "The Actor network is responsible for learning the optimal policy that maps states to actions. It has 3 layers:\n",
    "- Input layer: receives the state of the agent as input (size 24)\n",
    "- Hidden layer 1: This is a linear layer and has 256 output neurons and uses ReLU activation function\n",
    "- Hidden layer 2: This is a linear layer and has 128 output neurons and uses ReLU activation function\n",
    "- Output layer: has 2 neurons and uses tanh activation function to output the corresponding action\n",
    "\n",
    "The Critic network is responsible for evaluating the quality of the actions taken by the Actor network. It has 3 layers:\n",
    "- Input layer: receives all agents states as input (size 48)\n",
    "- Hidden layer 1: This is a linear layer and has 256 output neurons and uses ReLU activation function\n",
    "- Hidden layer 2: This is a linear layer, the input comes from the first layer and is also additionally concatenated with actions of all agents (size = 4)  and has 128 output neurons and uses ReLU activation function\n",
    "- Output layer: This is a linear layer and has 1 neuron and has no activation function to output the Q-value\n",
    "\n",
    "Both networks use the hidden_init function to initialize the weights of the hidden layers to prevent vanishing or exploding gradients during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters used:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chosen hyperparameters were based on prior experience and experimentation, which showed that they lead to stable training and acceptable performance for the defined problem.\n",
    "\n",
    "- BUFFER_SIZE: The size of the replay buffer. In this implementation, it is set to 1,000,000. This value is used to store the experiences and samples are drawn from it to train the neural network.\n",
    "\n",
    "- BATCH_SIZE: The size of the mini-batch sampled from the replay buffer for each training iteration. In this implementation, it is set to 128. A larger batch size can help with stability of the learning process, but also requires more memory.\n",
    "\n",
    "- GAMMA: The discount factor used to calculate the future rewards. In this implementation, it is set to 0.99, which means the agent highly values future rewards in comparison to immediate rewards.\n",
    "\n",
    "- TAU: The soft update parameter used to update the target network. In this implementation, it is set to 0.001, which means that the target network parameters are updated slowly by 0.1% towards the parameters of the main network during each training iteration.\n",
    "\n",
    "- LR_ACTOR: The learning rate used in the optimizer to update the parameters of the actor neural network. In this implementation, it is set to 0.001.\n",
    "\n",
    "- LR_CRITIC: The learning rate used in the optimizer to update the parameters of the critic neural network. In this implementation, it is set to 0.001.\n",
    "\n",
    "- WEIGHT_DECAY: The L2 weight decay regularization factor. In this implementation, it is set to 0. This generally helps to prevent overfitting during training by adding a penalty term to the loss function that encourages smaller weight values. However setting it to 0 has turned this off, as this allows for faster training and due to the model being complex due to being in a multi agent setting, it allows the model to better fit to the training data as the regularization penalty can sometimes prevent the model from fully exploiting the complexity of the data.\n",
    "\n",
    "- UPDATE_EVERY: The number of time steps after which the agent updates the neural network. In this implementation, it is set to 1. This means that the neural network is updated after every time step.\n",
    "\n",
    "- UPDATE_TIMES: The number of times the neural network is updated in each training iteration. In this implementation, it is set to 5. A higher value will result in more updates to the neural network which can improve the stability and convergence of the learning algorithm by reducing the impact of noisy updates. However, this comes at the cost of increased computational resources required to perform the additional updates, so there is a trade-off between training efficiency and computational cost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f2f2f2; display: inline-block;\">\n",
    "    <img src=\"./Plot_of_Rewards.png\" alt=\"Plot of Rewards\" style=\"vertical-align: left;\">\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Training_Episodes.png\" alt=\"Plot of Rewards\" style=\"vertical-align: right;\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, the environment was solved in 1968 episodes with an average Score of 0.5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-agent reinforcement learning presents a challenge in developing an optimal policy due to the complex interactions between agents. Therefore, a single policy may not be suitable for all agents. MADDPG is a powerful algorithm that addresses these challenges, but there are other algorithms that have been developed to further improve performance and address specific challenges.\n",
    "\n",
    "One such algorithm is Proximal Policy Optimization (PPO), a policy-based algorithm that uses a surrogate objective function to improve the policy update. PPO has been shown to provide stable and efficient learning in multi-agent settings and has been used successfully in a variety of environments. PPO has the added advantage of being compatible with continuous action spaces, which makes it a suitable choice for applications that require fine-grained control.\n",
    "\n",
    "Another algorithm is Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments (MAAC),  an actor-critic algorithm that uses a central critic to learn value estimates for each agent, while each agent has its own actor policy. MAAC has been shown to achieve state-of-the-art performance in a range of multi-agent environments, making it a promising option for Multi-agent reinforcement learning applications.\n",
    "\n",
    "However, when investigating the use of these algorithms, special attention must be given to their computational complexity and hyperparameter tuning. These factors can significantly impact the efficiency and effectiveness of the learning process, making it important to carefully choose the most appropriate algorithm for the specific problem at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
